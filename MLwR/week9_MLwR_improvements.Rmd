---
title: "Improving Model Performance"
author: "Emma Grossman"
date: "5/25/2021"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
library(tidyverse)
```

# Tuning stock models for better performance

For an example, we'll be returning to the `credit` data from chapter 5. Though the accuracy was about 82\%, the kappa statistic was only 0.28, which is fairly poor. 

The process of adjusting the model options to identify the best fit is called **parameter tuning**.

## Using caret for automated parameter tuning

> Rather than choosing arbitrary values for each of  the model's parameters - a task that is not only tedious but also somewhat unscientific - it is better to conduct a search through many possible parameter values to find the best combination. 

The `caret` package has tools that automate parameter tuning. 

Three questions generated by automated parameter tuning are:

1. What type of machine learning model (and specific implementation) should be trained on the data?
2. Which model parameters can be adjusted and how extensively should they be tuned to find the optimal settings?
3. What criteria should be used to evaluate the models to find the best candidate?


To view the tuning parameters for specific models, `modelLookup()` can be used.
```{r}
library(caret)
modelLookup("C5.0")
```

When parameter tuning, the `caret` package tries at most 3 values for each parameter. After fitting all of the combinations of parameters, the best model is identified. 

## Creating a simple tuned model

```{r}
credit <- read.csv("https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/credit.csv", header = TRUE)
credit <- credit %>%
  mutate(
    default = as.factor(ifelse(default==1,"no","yes")),
    existing_credits = as.factor(existing_credits),
    dependents = as.factor(dependents)
  )

RNGversion("3.5.2")
set.seed(300)
m <- caret::train(default~., data = credit, method = "C5.0")
```

```{r}
m
```

The best model is stored in `m` as `finalModel`, but we rarely need to access it directly.
```{r}
p <- predict(m, credit)
table(p, credit$default)
```

```{r}
head(predict(m, credit))
head(predict(m, credit, type = "prob"))
```

## Customizing the tuning process

A **control object** is a set of configuration options, the function in `caret` is `trainControl()`. It modifies a tuning experiment; parameters `method` (sets the resampling method) and `selectionFunction` (specifies the funtion that will choose the optimal model among various candidates) are particularly useful. 

```{r}
ctrl <- trainControl(method = "cv", number = 10, selectionFunction = "oneSE")
```


```{r}
grid <- expand.grid(model = "tree",
                    trials = c(1, seq(5, 35, by = 5)),
                    winnow = FALSE)

grid
```


```{r}
set.seed(300)
m <- train(default~., data = credit, method = "C5.0",
           metric = "Kappa",
           trControl = ctrl,
           tuneGrid = grid)
m
```

# Improving model performance with meta-learning

This involves combining several models to form a powerful team. **Meta-learning** methods are techniques that involve learning how to learn, and includes the task of combining and managing multiple models. 

## Understanding ensembles

> The meta-learninga pproach that utilizes a similar principle of creating a varied team of experts is known as an **ensemble**. 

Training data is split between models based on an **allocation function**.  A **combination function** determines how disagreements among model predictions are resolved.  **Stacking** is the method in which predictions of several models train a final arbiter model. 

Benefits of ensembles are:

- better generalizability to future problems
- improved performance on massive or minuscule datasets
- the ability to synthesize data from distinct domains
- a more nuanced understanding of difficult learning tasks

## Bagging

**Bootstrap aggregating** or **bagging** was one of the first ensemble methods to gain popularity and widespread usage after introduction in 1994. 

Bagging performs well when the models that comprise it are **unstable**, meaning they change substantially when input data changes slightly. An example of an unstable method is a decision tree. 

```{r}
library(ipred)
set.seed(300)
mybag <- bagging(default~., data = credit, nbagg = 25)
```

```{r}
credit_pred <- predict(mybag, credit)
table(credit_pred, credit$default)
```

```{r}
library(caret)
ctrl <- trainControl(method  = "cv", number = 10)
train(default~., data = credit, method = "treebag",
      trControl = ctrl)
```

## Boosting

**Boosting** is another common ensemble method, named such because it *boosts* the performance of weak learners. 

There are two main differences between bagging and boosting:

1. the resampled datasets in boosting are constructed specifically to generated complementary learners
2. each learner's vote is weighted on their past performance

**AdaBoost** or **adaptive boosting** trains data on difficult (hard to classify) examples and generally performs very well. 

```{r}
# install.packages("adabag")
library(adabag)
m_adaboost <- boosting(default~., data = credit)
```

```{r}
p_adaboost <- predict(m_adaboost, credit)
head(p_adaboost$class)
p_adaboost$confusion
```

Though this seems to have classified perfectly, it is based on the training data and is overfitting. We can combine this with k-fold CV:

```{r}
adaboost_cv <- boosting.cv(default~., data = credit)
adaboost_cv$confusion
```


```{r}
library(vcd)
Kappa(adaboost_cv$confusion)
```

## Random forests

This is another ensemble method specific to decision trees. It combines bagging methods with random feature selection. Only a small, random portion of the full feature set is used so the "curse of dimensionality" is not a problem. They are easier to use and don't overfit as much. 

Strengths: 

- performs well for most problems
- works well with noisy or missing data, categorical or continuous data
- selects only most important features
- used for extremely large number of features

Weaknesses:

- not easy to interpret

### Training random forests

```{r}
# install.packages("randomForest")
library(randomForest)
```


```{r}
rf <- randomForest(default~., data = credit)
rf
```

```{r}
Kappa(rf$confusion[1:2,1:2])
```

### Evaluating random forest performance in a simulated competition

```{r}
ctrl <- trainControl(method = "repeatedcv",
                     number = 10, repeats = 10,
                     selectionFunction = "best",
                     savePredictions = TRUE,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary)

grid_rf <- expand.grid(mtry = c(2, 4, 8, 16))
```

```{r}
set.seed(300)
m_rf <- train(default~., data = credit, method = "rf",
              metric = "ROC", trControl = ctrl,
              tuneGrid = grid_rf)
```


```{r}
grid_c50 <- expand.grid(model = "tree",
                        trials = c(10, 25, 50, 100),
                        winnow = FALSE)
set.seed(300)
m_c50 <- train(default~., data = credit, method = "C5.0",
              metric = "ROC", trControl = ctrl,
              tuneGrid = grid_c50)
```


```{r}
m_rf
m_c50
```


```{r}
library(pROC)
roc_rf <- roc(m_rf$pred$obs, m_rf$pred$yes)
roc_c50 <- roc(m_c50$pred$obs, m_c50$pred$yes)
plot(roc_rf, col = "red", legacy.axes = TRUE)
plot(roc_c50, col = "blue", add = TRUE)
```





